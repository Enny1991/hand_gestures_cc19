{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dnn_fusion.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQUy6PheE2Sn",
        "colab_type": "code",
        "outputId": "ba32eae0-e9bf-4b6e-909f-294a18e02765",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# install PyDrive\n",
        "!pip install -U -q PyDrive\n",
        "\n",
        "# imports\n",
        "from __future__ import print_function\n",
        "import os\n",
        "import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle as pkl\n",
        "from tensorflow.contrib import lite\n",
        "from keras.models import Sequential, load_model, Model\n",
        "from keras.layers import Dense, Dropout, Flatten, Add, Concatenate, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, AveragePooling1D, AveragePooling2D\n",
        "from keras import backend as K\n",
        "from scipy.signal import stft\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# authenticate and create the PyDrive client\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zq5Y8a8E3Zl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# links for data loading\n",
        "link_img = \"https://drive.google.com/open?id=1lAxQhHf8oNjTZ84_empUhcaZOL2lWqwu\"\n",
        "link_emg = \"https://drive.google.com/open?id=181QG5teO-btOlGMBkD6negYWBz2M6XKr\"\n",
        "link_lbl = \"https://drive.google.com/open?id=1k5UIqXI_pdu99j1zk1C2Rn7jnsGdgQs_\"\n",
        "link_frm = \"https://drive.google.com/open?id=1iQyZL8z0A12iDQqOR-O2PvUcrf9h9Q31\"\n",
        "link_evt = \"https://drive.google.com/open?id=1YWR4HqWiK3EU7ZVCTsXefVKpH7R8d8QC\"\n",
        "link_100 = \"https://drive.google.com/open?id=1lTS3kSo_M2Zj3NoRB_pxS7GRgqj_i0fu\"\n",
        "link_150 = \"https://drive.google.com/open?id=1RWBe7MOEkDM8Jjcs5NaGbXe_LdoHar43\"\n",
        "link_200 = \"https://drive.google.com/open?id=1sfaVY7VXXUD3x0ak8iyUIko217Dwrivd\"\n",
        "link_250 = \"https://drive.google.com/open?id=1R9481MZIqPxr40wLoCB4nJAtTKZRiBni\"\n",
        "\n",
        "# names for data loading\n",
        "name_img = \"cc19_hand_gestures_10p_img.npy\"\n",
        "name_emg = \"cc19_hand_gestures_10p_emg.npy\"\n",
        "name_lbl = \"cc19_hand_gestures_10p_lbl.npy\"\n",
        "name_frm = \"full_dataset_frames.pkl\"\n",
        "name_evt = \"full_dataset_events.pkl\"\n",
        "name_100 = \"fusion_features_100ms.pkl\"\n",
        "name_150 = \"fusion_features_150ms.pkl\"\n",
        "name_200 = \"fusion_features_200ms.pkl\"\n",
        "name_250 = \"fusion_features_250ms.pkl\"\n",
        "\n",
        "# names for h5 data saving\n",
        "name_emg = \"cnn_emg.h5\"\n",
        "name_dvs_evs = \"cnn_dvs_evs.h5\"\n",
        "name_dav_evs = \"cnn_dav_evs.h5\"\n",
        "name_dav_aps = \"cnn_dav_aps.h5\"\n",
        "name_emg_dvs_evs = \"cnn_emg+dvs_evs.h5\"\n",
        "name_emg_dav_evs = \"cnn_emg+dav_evs.h5\"\n",
        "name_emg_dav_aps = \"cnn_emg+dav_aps.h5\"\n",
        "\n",
        "# names for tflite data saving\n",
        "tflite_emg = \"cnn_emg.tflite\"\n",
        "tflite_dvs_evs = \"cnn_dvs_evs.tflite\"\n",
        "tflite_dav_evs = \"cnn_dav_evs.tflite\"\n",
        "tflite_dav_aps = \"cnn_dav_aps.tflite\"\n",
        "tflite_emg_dvs_evs = \"cnn_emg+dvs_evs.tflite\"\n",
        "tflite_emg_dav_evs = \"cnn_emg+dav_evs.tflite\"\n",
        "tflite_emg_dav_aps = \"cnn_emg+dav_aps.tflite\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2x9x0AGPR1N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# hyper-parameters definition\n",
        "data_base = 200\n",
        "k_splits = 5\n",
        "batch_size = 128\n",
        "num_classes = 5\n",
        "div_epochs = 50\n",
        "emg_epochs = 50\n",
        "fus_epochs = 50\n",
        "seed = 23\n",
        "np.random.seed(seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmnSdU5oB_yS",
        "colab_type": "code",
        "outputId": "c5496b5c-8f41-43bd-a831-95936e1da4f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "# fuction to load data\n",
        "def load_data(link, name):\n",
        "    fluff, id = link.split('=')\n",
        "    print(\"--- loading data: \" + name + \" from id -> \" + id + \" ---\")\n",
        "    downloaded = drive.CreateFile({\"id\":id}) \n",
        "    downloaded.GetContentFile(name)  \n",
        "    data = np.load(name, allow_pickle=True)\n",
        "    \n",
        "    labels = data[\"lbl\"]\n",
        "    emg = data[\"emg\"]\n",
        "    dvs_evs = data[\"dvs_evs_frames\"]\n",
        "    dav_evs = data[\"dav_evs_frames\"]\n",
        "    dav_aps = data[\"dav_aps_frames\"]\n",
        "    \n",
        "    print(\"labels.shape = \", labels.shape)\n",
        "    print(\"emg.shape = \", emg.shape)\n",
        "    print(\"dvs_evs.shape = \", dvs_evs.shape)\n",
        "    print(\"dav_evs.shape = \", dav_evs.shape)\n",
        "    print(\"dav_aps.shape = \", dav_aps.shape)\n",
        "    print(\"\\n\")\n",
        "    \n",
        "    return labels, emg, dvs_evs, dav_evs, dav_aps\n",
        "\n",
        "# load data\n",
        "if data_base == 100:\n",
        "    data_labels, data_emg, data_dvs_evs, data_dav_evs, data_dav_aps = load_data(link_100, name_100)\n",
        "elif data_base == 150:\n",
        "    data_labels, data_emg, data_dvs_evs, data_dav_evs, data_dav_aps = load_data(link_150, name_150)\n",
        "elif data_base == 200:\n",
        "    data_labels, data_emg, data_dvs_evs, data_dav_evs, data_dav_aps = load_data(link_200, name_200)\n",
        "elif data_base == 250:\n",
        "    data_labels, data_emg, data_dvs_evs, data_dav_evs, data_dav_aps = load_data(link_250, name_250)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- loading data: fusion_features_200ms.pkl from id -> 1sfaVY7VXXUD3x0ak8iyUIko217Dwrivd ---\n",
            "labels.shape =  (6750, 1)\n",
            "emg.shape =  (6750, 16)\n",
            "dvs_evs.shape =  (6750, 60, 60)\n",
            "dav_evs.shape =  (6750, 60, 60)\n",
            "dav_aps.shape =  (6750, 60, 60)\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZ19_cnfQ7Nm",
        "colab_type": "code",
        "outputId": "dee0c4b7-fb9c-4d44-a592-74104210df3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 823
        }
      },
      "source": [
        "# function to normalize and standardize data\n",
        "def norm_stand(data):\n",
        "    print(\"--- normalize and standardize data ---\")\n",
        "    \n",
        "    print(\"data_max = \", np.max(data))\n",
        "    print(\"data_min = \", np.min(data))\n",
        "    print(\"data_mean = \", np.mean(data))\n",
        "    print(\"data_std = \", np.std(data))\n",
        "    \n",
        "    data = data.astype(float)\n",
        "\n",
        "    # normalize\n",
        "    data_max = np.max(data)\n",
        "    data_min = np.min(data)\n",
        "    for i in range(len(data)):\n",
        "        data[i] = (data[i] - data_min) / (data_max - data_min)\n",
        "        \n",
        "    # standardize\n",
        "    data_mean = np.mean(data)\n",
        "    data_std = np.std(data) + 1e-15\n",
        "    data -= data_mean\n",
        "    data /= data_std\n",
        "    \n",
        "    print(\"---\")\n",
        "    print(\"new_data_max = \", np.max(data))\n",
        "    print(\"new_data_min = \", np.min(data))\n",
        "    print(\"new_data_mean = \", np.mean(data))\n",
        "    print(\"new_data_std = \", np.std(data))\n",
        "    print(\"\\n\")\n",
        "    \n",
        "    return data\n",
        "    \n",
        "# normalize and standardize data\n",
        "data_emg = norm_stand(data_emg)\n",
        "data_dvs_evs = norm_stand(data_dvs_evs)\n",
        "data_dav_evs = norm_stand(data_dav_evs)\n",
        "data_dav_aps = norm_stand(data_dav_aps)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- normalize and standardize data ---\n",
            "data_max =  68.71754309696473\n",
            "data_min =  0.0\n",
            "data_mean =  9.989849023500414\n",
            "data_std =  8.00750450155642\n",
            "---\n",
            "new_data_max =  7.334081930525158\n",
            "new_data_min =  -1.2475608376565506\n",
            "new_data_mean =  -1.1579214953127558e-17\n",
            "new_data_std =  0.9999999999999913\n",
            "\n",
            "\n",
            "--- normalize and standardize data ---\n",
            "data_max =  255\n",
            "data_min =  0\n",
            "data_mean =  5.02497621399177\n",
            "data_std =  18.176710732725073\n",
            "---\n",
            "new_data_max =  13.752489515936109\n",
            "new_data_min =  -0.27645134963526835\n",
            "new_data_mean =  -6.117059488114522e-16\n",
            "new_data_std =  0.9999999999999869\n",
            "\n",
            "\n",
            "--- normalize and standardize data ---\n",
            "data_max =  240\n",
            "data_min =  0\n",
            "data_mean =  12.66215390946502\n",
            "data_std =  23.17336329401839\n",
            "---\n",
            "new_data_max =  9.810308637814954\n",
            "new_data_min =  -0.5464098477553875\n",
            "new_data_mean =  6.146270689473549e-16\n",
            "new_data_std =  0.9999999999999903\n",
            "\n",
            "\n",
            "--- normalize and standardize data ---\n",
            "data_max =  250\n",
            "data_min =  2\n",
            "data_mean =  184.0460073251029\n",
            "data_std =  46.54963776709588\n",
            "---\n",
            "new_data_max =  1.4168529732688349\n",
            "new_data_min =  -3.9107932103777374\n",
            "new_data_mean =  5.685717356463112e-15\n",
            "new_data_std =  0.9999999999999971\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYdNZxQe13Qw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data format convertion\n",
        "data_labels = keras.utils.to_categorical(data_labels, num_classes)\n",
        "data_emg = data_emg.reshape(data_emg.shape[0], data_emg.shape[1], 1)\n",
        "data_dvs_evs = data_dvs_evs.reshape(data_dvs_evs.shape[0], data_dvs_evs.shape[1], data_dvs_evs.shape[2], 1)\n",
        "data_dav_evs = data_dav_evs.reshape(data_dav_evs.shape[0], data_dav_evs.shape[1], data_dav_evs.shape[2], 1)\n",
        "data_dav_aps = data_dav_aps.reshape(data_dvs_evs.shape[0], data_dvs_evs.shape[1], data_dvs_evs.shape[2], 1)\n",
        "\n",
        "vid_input_shape = (data_dvs_evs.shape[1], data_dvs_evs.shape[2], 1)\n",
        "emg_input_shape = (data_emg.shape[1], 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUPs8yAvyzyJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to shuffle data\n",
        "def unison_shuffled_copies(a, b, c, d, e):\n",
        "    p = np.random.permutation(len(a))\n",
        "    return a[p], b[p], c[p], d[p], e[p]\n",
        "\n",
        "# shuffle data\n",
        "data_labels, data_emg, data_dvs_evs, data_dav_evs, data_dav_aps = unison_shuffled_copies(data_labels, data_emg, data_dvs_evs, data_dav_evs, data_dav_aps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRS6V6wg42cR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cnn model for vid\n",
        "def create_vid_lenet():\n",
        "    print(\"--- vid_cnn: lenet ---\")\n",
        "    model = keras.Sequential()\n",
        "    model.add(Conv2D(filters=6, kernel_size=(3, 3), activation='relu', input_shape=vid_input_shape))\n",
        "    model.add(AveragePooling2D())\n",
        "    model.add(Conv2D(filters=16, kernel_size=(3, 3), activation='relu'))\n",
        "    model.add(AveragePooling2D())\n",
        "    model.add(Conv2D(filters=120, kernel_size=(5, 5), activation='relu'))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(units=84, activation='relu'))\n",
        "    model.add(Dense(units=num_classes, activation = 'softmax'))\n",
        "    \n",
        "    #model.summary()\n",
        "    \n",
        "    return model\n",
        "  \n",
        "def create_vid_cc19():\n",
        "    print(\"--- vid_cnn: cc19 ---\")\n",
        "    model_img = Sequential()\n",
        "    model_img.add(Conv2D(filters=8, kernel_size=(3, 3), activation='relu', input_shape=vid_input_shape))\n",
        "    model_img.add(Conv2D(filters=8, kernel_size=(3, 3), activation='relu'))\n",
        "    model_img.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model_img.add(Dropout(0.25))\n",
        "    model_img.add(Flatten())\n",
        "    model_img.add(Dense(50, activation='relu'))\n",
        "    model_img.add(Dropout(0.5))\n",
        "    model_img.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    #model_img.summary()\n",
        "    \n",
        "    return model_img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5LvC-TIVSEt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cnn model for emg\n",
        "def create_emg_lenet():\n",
        "    print(\"--- emg_cnn: lenet ---\")\n",
        "    model = keras.Sequential()\n",
        "    model.add(Conv1D(filters=6, kernel_size=3, activation='relu', input_shape=emg_input_shape))\n",
        "    model.add(Conv1D(filters=16, kernel_size=3, activation='relu'))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(units=120, activation='relu'))\n",
        "    model.add(Dense(units=84, activation='relu'))\n",
        "    model.add(Dense(units=num_classes, activation = 'softmax'))\n",
        "    \n",
        "    #model.summary()\n",
        "    \n",
        "    return model\n",
        "\n",
        "def create_emg_cc19():\n",
        "    print(\"--- emg_cnn: cc19 ---\")\n",
        "    model_emg = Sequential()\n",
        "    model_emg.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=emg_input_shape))\n",
        "    model_emg.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
        "    model_emg.add(Dropout(0.5))\n",
        "    model_emg.add(MaxPooling1D(pool_size=2))\n",
        "    model_emg.add(Flatten())\n",
        "    model_emg.add(Dense(100, activation='relu'))\n",
        "    model_emg.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    #model_emg.summary()\n",
        "    \n",
        "    return model_emg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KR45Xub8vXYa",
        "colab_type": "code",
        "outputId": "68471d03-1f69-476a-ad6f-820f1627aed2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        }
      },
      "source": [
        "### data_dvs_evs ###\n",
        "\n",
        "# evaluate a single model\n",
        "def evaluate_dvs_evs(trainX, trainy, testX, testy):\n",
        "    # compile the model\n",
        "    model_dvs_evs.compile(loss=keras.losses.categorical_crossentropy,\n",
        "                optimizer=keras.optimizers.Adadelta(),\n",
        "                metrics=['accuracy'])\n",
        "    \n",
        "    # fit the model\n",
        "    model_dvs_evs.fit(trainX, trainy,\n",
        "            batch_size=batch_size,\n",
        "            epochs=div_epochs,\n",
        "            verbose=0,\n",
        "            validation_data=(testX, testy))\n",
        "    \n",
        "    # evaluate the model\n",
        "    scores, test_acc = model_dvs_evs.evaluate(testX, testy, verbose=1)\n",
        "    #print(\"%s: %.2f%%\" % (model_vid.metrics_names[1], scores[1]*100))\n",
        "    \n",
        "    return model_dvs_evs, test_acc\n",
        "\n",
        "# prepare the k-fold cross-validation configuration\n",
        "n_folds = 5\n",
        "kfold = KFold(n_splits = k_splits, shuffle=False, random_state=None)\n",
        "\n",
        "# cross validation estimation of performance\n",
        "scores, members = list(), list()\n",
        "for train_ix, test_ix in kfold.split(data_dvs_evs):\n",
        "    # reset variables\n",
        "    %reset_selective -f model_div\n",
        "    \n",
        "    # select samples\n",
        "    trainX, trainy = data_dvs_evs[train_ix], data_labels[train_ix]\n",
        "    testX, testy = data_dvs_evs[test_ix], data_labels[test_ix]\n",
        "    \n",
        "    # create model\n",
        "    model_dvs_evs = create_vid_lenet()\n",
        "    \n",
        "    # evaluate model\n",
        "    model, test_acc = evaluate_dvs_evs(trainX, trainy, testX, testy)\n",
        "    print('>%.3f' % test_acc)\n",
        "    scores.append(test_acc)\n",
        "    members.append(model)\n",
        "\n",
        "# summarize expected performance\n",
        "print('Estimated Accuracy %.3f (%.3f)' % (np.mean(scores), np.std(scores)))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- vid_cnn: lenet ---\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "1350/1350 [==============================] - 0s 72us/step\n",
            ">0.893\n",
            "--- vid_cnn: lenet ---\n",
            "1350/1350 [==============================] - 0s 78us/step\n",
            ">0.901\n",
            "--- vid_cnn: lenet ---\n",
            "1350/1350 [==============================] - 0s 67us/step\n",
            ">0.893\n",
            "--- vid_cnn: lenet ---\n",
            "1350/1350 [==============================] - 0s 76us/step\n",
            ">0.902\n",
            "--- vid_cnn: lenet ---\n",
            "1350/1350 [==============================] - 0s 68us/step\n",
            ">0.902\n",
            "Estimated Accuracy 0.899 (0.004)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvzJTtxaUXk4",
        "colab_type": "code",
        "outputId": "6355aac3-0066-4109-aae4-e38d160fe81c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "### data_dav_evs ###\n",
        "\n",
        "# evaluate a single model\n",
        "def evaluate_model(trainX, trainy, testX, testy):\n",
        "    # compile the model\n",
        "    model_dav_evs.compile(loss=keras.losses.categorical_crossentropy,\n",
        "                optimizer=keras.optimizers.Adadelta(),\n",
        "                metrics=['accuracy'])\n",
        "    \n",
        "    # fit the model\n",
        "    model_dav_evs.fit(trainX, trainy,\n",
        "            batch_size=batch_size,\n",
        "            epochs=div_epochs,\n",
        "            verbose=0,\n",
        "            validation_data=(testX, testy))\n",
        "    \n",
        "    # evaluate the model\n",
        "    scores, test_acc = model_dav_evs.evaluate(testX, testy, verbose=1)\n",
        "    #print(\"%s: %.2f%%\" % (model_vid.metrics_names[1], scores[1]*100))\n",
        "    \n",
        "    return model_dav_evs, test_acc\n",
        "\n",
        "# prepare the k-fold cross-validation configuration\n",
        "n_folds = 5\n",
        "kfold = KFold(n_splits = k_splits, shuffle=False, random_state=None)\n",
        "\n",
        "# cross validation estimation of performance\n",
        "scores, members = list(), list()\n",
        "for train_ix, test_ix in kfold.split(data_dav_evs):\n",
        "    # reset variables\n",
        "    %reset_selective -f model_div\n",
        "    \n",
        "    # select samples\n",
        "    trainX, trainy = data_dav_evs[train_ix], data_labels[train_ix]\n",
        "    testX, testy = data_dav_evs[test_ix], data_labels[test_ix]\n",
        "    \n",
        "    # create model\n",
        "    model_dav_evs = create_vid_lenet()\n",
        "    \n",
        "    # evaluate model\n",
        "    model, test_acc = evaluate_model(trainX, trainy, testX, testy)\n",
        "    print('>%.3f' % test_acc)\n",
        "    scores.append(test_acc)\n",
        "    members.append(model)\n",
        "\n",
        "# summarize expected performance\n",
        "print('Estimated Accuracy %.3f (%.3f)' % (np.mean(scores), np.std(scores)))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- vid_cnn: lenet ---\n",
            "1350/1350 [==============================] - 0s 81us/step\n",
            ">0.901\n",
            "--- vid_cnn: lenet ---\n",
            "1350/1350 [==============================] - 0s 71us/step\n",
            ">0.909\n",
            "--- vid_cnn: lenet ---\n",
            "1350/1350 [==============================] - 0s 72us/step\n",
            ">0.931\n",
            "--- vid_cnn: lenet ---\n",
            "1350/1350 [==============================] - 0s 67us/step\n",
            ">0.909\n",
            "--- vid_cnn: lenet ---\n",
            "1350/1350 [==============================] - 0s 68us/step\n",
            ">0.916\n",
            "Estimated Accuracy 0.913 (0.010)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYpesTQbUgP8",
        "colab_type": "code",
        "outputId": "e2f14e55-3bd2-48d7-f8b8-99c734d73945",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "### data_dav_aps ###\n",
        "\n",
        "# evaluate a single model\n",
        "def evaluate_model(trainX, trainy, testX, testy):\n",
        "    # compile the model\n",
        "    model_dav_aps.compile(loss=keras.losses.categorical_crossentropy,\n",
        "                optimizer=keras.optimizers.Adadelta(),\n",
        "                metrics=['accuracy'])\n",
        "    \n",
        "    # fit the model\n",
        "    model_dav_aps.fit(trainX, trainy,\n",
        "            batch_size=batch_size,\n",
        "            epochs=div_epochs,\n",
        "            verbose=0,\n",
        "            validation_data=(testX, testy))\n",
        "    \n",
        "    # evaluate the model\n",
        "    scores, test_acc = model_dav_aps.evaluate(testX, testy, verbose=1)\n",
        "    #print(\"%s: %.2f%%\" % (model_vid.metrics_names[1], scores[1]*100))\n",
        "    \n",
        "    return model_dav_aps, test_acc\n",
        "\n",
        "# prepare the k-fold cross-validation configuration\n",
        "n_folds = 5\n",
        "kfold = KFold(n_splits = k_splits, shuffle=False, random_state=None)\n",
        "\n",
        "# cross validation estimation of performance\n",
        "scores, members = list(), list()\n",
        "for train_ix, test_ix in kfold.split(data_dav_aps):\n",
        "    # reset variables\n",
        "    %reset_selective -f model_div\n",
        "    \n",
        "    # select samples\n",
        "    trainX, trainy = data_dav_aps[train_ix], data_labels[train_ix]\n",
        "    testX, testy = data_dav_aps[test_ix], data_labels[test_ix]\n",
        "    \n",
        "    # create model\n",
        "    model_dav_aps = create_vid_lenet()\n",
        "    \n",
        "    # evaluate model\n",
        "    model, test_acc = evaluate_model(trainX, trainy, testX, testy)\n",
        "    print('>%.3f' % test_acc)\n",
        "    scores.append(test_acc)\n",
        "    members.append(model)\n",
        "\n",
        "# summarize expected performance\n",
        "print('Estimated Accuracy %.3f (%.3f)' % (np.mean(scores), np.std(scores)))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- vid_cnn: lenet ---\n",
            "1350/1350 [==============================] - 0s 82us/step\n",
            ">0.918\n",
            "--- vid_cnn: lenet ---\n",
            "1350/1350 [==============================] - 0s 68us/step\n",
            ">0.930\n",
            "--- vid_cnn: lenet ---\n",
            "1350/1350 [==============================] - 0s 78us/step\n",
            ">0.919\n",
            "--- vid_cnn: lenet ---\n",
            "1350/1350 [==============================] - 0s 81us/step\n",
            ">0.926\n",
            "--- vid_cnn: lenet ---\n",
            "1350/1350 [==============================] - 0s 72us/step\n",
            ">0.919\n",
            "Estimated Accuracy 0.922 (0.005)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVi3hRuHVt88",
        "colab_type": "code",
        "outputId": "84d40f7d-92c4-4442-a36c-8b4c6077f0d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "### data_emg ###\n",
        "\n",
        "# evaluate a single model\n",
        "def evaluate_emg(trainX, trainy, testX, testy):\n",
        "    # compile the model\n",
        "    model_emg.compile(loss=keras.losses.categorical_crossentropy,\n",
        "                optimizer=keras.optimizers.Adadelta(),\n",
        "                metrics=['accuracy'])\n",
        "    \n",
        "    # fit the model\n",
        "    model_emg.fit(trainX, trainy,\n",
        "            batch_size=batch_size,\n",
        "            epochs=div_epochs,\n",
        "            verbose=0,\n",
        "            validation_data=(testX, testy))\n",
        "    \n",
        "    # evaluate the model\n",
        "    scores, test_acc = model_emg.evaluate(testX, testy, verbose=1)\n",
        "    #print(\"%s: %.2f%%\" % (model_vid.metrics_names[1], scores[1]*100))\n",
        "    \n",
        "    return model_emg, test_acc\n",
        "\n",
        "# prepare the k-fold cross-validation configuration\n",
        "n_folds = 5\n",
        "kfold = KFold(n_splits = k_splits, shuffle=False, random_state=None)\n",
        "\n",
        "# cross validation estimation of performance\n",
        "scores, members = list(), list()\n",
        "for train_ix, test_ix in kfold.split(data_emg):\n",
        "    # reset variables\n",
        "    %reset_selective -f model_emg\n",
        "    \n",
        "    # select samples\n",
        "    trainX, trainy = data_emg[train_ix], data_labels[train_ix]\n",
        "    testX, testy = data_emg[test_ix], data_labels[test_ix]\n",
        "    \n",
        "    # create model\n",
        "    model_emg = create_emg_lenet()\n",
        "    \n",
        "    # evaluate model\n",
        "    model, test_acc = evaluate_emg(trainX, trainy, testX, testy)\n",
        "    print('>%.3f' % test_acc)\n",
        "    scores.append(test_acc)\n",
        "    members.append(model)\n",
        "\n",
        "# summarize expected performance\n",
        "print('Estimated Accuracy %.3f (%.3f)' % (np.mean(scores), np.std(scores)))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- emg_cnn: lenet ---\n",
            "1350/1350 [==============================] - 0s 55us/step\n",
            ">0.814\n",
            "--- emg_cnn: lenet ---\n",
            "1350/1350 [==============================] - 0s 54us/step\n",
            ">0.832\n",
            "--- emg_cnn: lenet ---\n",
            "1350/1350 [==============================] - 0s 52us/step\n",
            ">0.832\n",
            "--- emg_cnn: lenet ---\n",
            "1350/1350 [==============================] - 0s 52us/step\n",
            ">0.831\n",
            "--- emg_cnn: lenet ---\n",
            "1350/1350 [==============================] - 0s 52us/step\n",
            ">0.817\n",
            "Estimated Accuracy 0.825 (0.008)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wun7CCF9NF9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# mlp model for fus\n",
        "def create_mlp_fus(model_vid, model_emg):\n",
        "    print(\"--- fus_mlp ---\")\n",
        "    mergedOut = Concatenate()([model_vid.output, model_emg.output])\n",
        "    mergedOut = Dense(5, activation='softmax')(mergedOut)\n",
        "    model_fus = Model([model_vid.input, model_emg.input], mergedOut)\n",
        "    \n",
        "    # freeze the layers except the last dense\n",
        "    for layer in model_fus.layers[:len(model_fus.layers)-1]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # check the status of the layers\n",
        "    \"\"\"\n",
        "    for layer in model_fus.layers:\n",
        "        print(layer, layer.trainable)\n",
        "    model_fus.summary()\n",
        "    \"\"\"\n",
        "    \n",
        "    return model_fus"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ajir9AHekP94",
        "colab_type": "code",
        "outputId": "bb018079-bd84-4e91-c9a5-9ac6ee150b46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "### data_emg + data_dvs_evs ###\n",
        "\n",
        "# evaluate a single model\n",
        "def evaluate_emg_dvs_evs(trainX, trainy, testX, testy):\n",
        "    # compile the model\n",
        "    model_emg_dvs_evs.compile(loss=keras.losses.categorical_crossentropy,\n",
        "                optimizer=keras.optimizers.Adadelta(),\n",
        "                metrics=['accuracy'])\n",
        "    \n",
        "    # fit the model\n",
        "    model_emg_dvs_evs.fit(trainX, trainy,\n",
        "            batch_size=batch_size,\n",
        "            epochs=div_epochs,\n",
        "            verbose=0,\n",
        "            validation_data=(testX, testy))\n",
        "    \n",
        "    # evaluate the model\n",
        "    scores, test_acc = model_emg_dvs_evs.evaluate(testX, testy, verbose=1)\n",
        "    #print(\"%s: %.2f%%\" % (model_vid.metrics_names[1], scores[1]*100))\n",
        "    \n",
        "    return model_emg_dvs_evs, test_acc\n",
        "\n",
        "# prepare the k-fold cross-validation configuration\n",
        "kfold = KFold(n_splits = k_splits, shuffle=False, random_state=None)\n",
        "\n",
        "# cross validation estimation of performance\n",
        "scores, members = list(), list()\n",
        "for train_ix, test_ix in kfold.split(data_emg):\n",
        "    # reset variables\n",
        "    %reset_selective -f model_emg_dvs_evs\n",
        "    \n",
        "    # select samples\n",
        "    trainX, trainy = [data_dvs_evs[train_ix], data_emg[train_ix]], data_labels[train_ix]\n",
        "    testX, testy = [data_dvs_evs[test_ix], data_emg[test_ix]], data_labels[test_ix]\n",
        "    \n",
        "    # create model\n",
        "    model_emg_dvs_evs = create_mlp_fus(model_dvs_evs, model_emg)\n",
        "    \n",
        "    # evaluate model\n",
        "    model, test_acc = evaluate_emg_dvs_evs(trainX, trainy, testX, testy)\n",
        "    print('>%.3f' % test_acc)\n",
        "    scores.append(test_acc)\n",
        "    members.append(model)\n",
        "\n",
        "# summarize expected performance\n",
        "print('Estimated Accuracy %.3f (%.3f)' % (np.mean(scores), np.std(scores)))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- fus_mlp ---\n",
            "1350/1350 [==============================] - 0s 94us/step\n",
            ">1.000\n",
            "--- fus_mlp ---\n",
            "1350/1350 [==============================] - 0s 93us/step\n",
            ">1.000\n",
            "--- fus_mlp ---\n",
            "1350/1350 [==============================] - 0s 107us/step\n",
            ">1.000\n",
            "--- fus_mlp ---\n",
            "1350/1350 [==============================] - 0s 95us/step\n",
            ">1.000\n",
            "--- fus_mlp ---\n",
            "1350/1350 [==============================] - 0s 94us/step\n",
            ">0.913\n",
            "Estimated Accuracy 0.983 (0.035)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gP5KFvsnp-RD",
        "colab_type": "code",
        "outputId": "d2bc4a08-b0cf-43ab-9c0a-c4a4cb40253e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "### data_emg + data_dav_evs ###\n",
        "\n",
        "# evaluate a single model\n",
        "def evaluate_emg_dav_evs(trainX, trainy, testX, testy):\n",
        "    # compile the model\n",
        "    model_emg_dav_evs.compile(loss=keras.losses.categorical_crossentropy,\n",
        "                optimizer=keras.optimizers.Adadelta(),\n",
        "                metrics=['accuracy'])\n",
        "    \n",
        "    # fit the model\n",
        "    model_emg_dav_evs.fit(trainX, trainy,\n",
        "            batch_size=batch_size,\n",
        "            epochs=div_epochs,\n",
        "            verbose=0,\n",
        "            validation_data=(testX, testy))\n",
        "    \n",
        "    # evaluate the model\n",
        "    scores, test_acc = model_emg_dav_evs.evaluate(testX, testy, verbose=1)\n",
        "    #print(\"%s: %.2f%%\" % (model_vid.metrics_names[1], scores[1]*100))\n",
        "    \n",
        "    return model_emg_dav_evs, test_acc\n",
        "\n",
        "# prepare the k-fold cross-validation configuration\n",
        "n_folds = 5\n",
        "kfold = KFold(n_splits = k_splits, shuffle=False, random_state=None)\n",
        "\n",
        "# cross validation estimation of performance\n",
        "scores, members = list(), list()\n",
        "for train_ix, test_ix in kfold.split(data_emg):\n",
        "    # reset variables\n",
        "    %reset_selective -f model_emg_dav_evs\n",
        "    \n",
        "    # select samples\n",
        "    trainX, trainy = [data_dav_evs[train_ix], data_emg[train_ix]], data_labels[train_ix]\n",
        "    testX, testy = [data_dav_evs[test_ix], data_emg[test_ix]], data_labels[test_ix]\n",
        "    \n",
        "    # create model\n",
        "    model_emg_dav_evs = create_mlp_fus(model_dav_evs, model_emg)\n",
        "    \n",
        "    # evaluate model\n",
        "    model, test_acc = evaluate_emg_dav_evs(trainX, trainy, testX, testy)\n",
        "    print('>%.3f' % test_acc)\n",
        "    scores.append(test_acc)\n",
        "    members.append(model)\n",
        "\n",
        "# summarize expected performance\n",
        "print('Estimated Accuracy %.3f (%.3f)' % (np.mean(scores), np.std(scores)))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- fus_mlp ---\n",
            "1350/1350 [==============================] - 0s 95us/step\n",
            ">1.000\n",
            "--- fus_mlp ---\n",
            "1350/1350 [==============================] - 0s 96us/step\n",
            ">1.000\n",
            "--- fus_mlp ---\n",
            "1350/1350 [==============================] - 0s 94us/step\n",
            ">1.000\n",
            "--- fus_mlp ---\n",
            "1350/1350 [==============================] - 0s 95us/step\n",
            ">1.000\n",
            "--- fus_mlp ---\n",
            "1350/1350 [==============================] - 0s 94us/step\n",
            ">0.931\n",
            "Estimated Accuracy 0.986 (0.028)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoQLSm2RqTTS",
        "colab_type": "code",
        "outputId": "bfe3c97a-d4aa-49d8-e56e-ea21ec2ea3d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "### data_emg + data_dav_aps ###\n",
        "\n",
        "# evaluate a single model\n",
        "def evaluate_emg_dav_aps(trainX, trainy, testX, testy):\n",
        "    # compile the model\n",
        "    model_emg_dav_aps.compile(loss=keras.losses.categorical_crossentropy,\n",
        "                optimizer=keras.optimizers.Adadelta(),\n",
        "                metrics=['accuracy'])\n",
        "    \n",
        "    # fit the model\n",
        "    model_emg_dav_aps.fit(trainX, trainy,\n",
        "            batch_size=batch_size,\n",
        "            epochs=div_epochs,\n",
        "            verbose=0,\n",
        "            validation_data=(testX, testy))\n",
        "    \n",
        "    # evaluate the model\n",
        "    scores, test_acc = model_emg_dav_aps.evaluate(testX, testy, verbose=1)\n",
        "    #print(\"%s: %.2f%%\" % (model_vid.metrics_names[1], scores[1]*100))\n",
        "    \n",
        "    return model_emg_dav_aps, test_acc\n",
        "\n",
        "# prepare the k-fold cross-validation configuration\n",
        "n_folds = 5\n",
        "kfold = KFold(n_splits = k_splits, shuffle=False, random_state=None)\n",
        "\n",
        "# cross validation estimation of performance\n",
        "scores, members = list(), list()\n",
        "for train_ix, test_ix in kfold.split(data_emg):\n",
        "    # reset variables\n",
        "    %reset_selective -f model_emg_dav_aps\n",
        "    \n",
        "    # select samples\n",
        "    trainX, trainy = [data_dav_aps[train_ix], data_emg[train_ix]], data_labels[train_ix]\n",
        "    testX, testy = [data_dav_aps[test_ix], data_emg[test_ix]], data_labels[test_ix]\n",
        "    \n",
        "    # create model\n",
        "    model_emg_dav_aps = create_mlp_fus(model_dav_aps, model_emg)\n",
        "    \n",
        "    # evaluate model\n",
        "    model, test_acc = evaluate_emg_dav_aps(trainX, trainy, testX, testy)\n",
        "    print('>%.3f' % test_acc)\n",
        "    scores.append(test_acc)\n",
        "    members.append(model)\n",
        "\n",
        "# summarize expected performance\n",
        "print('Estimated Accuracy %.3f (%.3f)' % (np.mean(scores), np.std(scores)))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- fus_mlp ---\n",
            "1350/1350 [==============================] - 0s 92us/step\n",
            ">1.000\n",
            "--- fus_mlp ---\n",
            "1350/1350 [==============================] - 0s 96us/step\n",
            ">1.000\n",
            "--- fus_mlp ---\n",
            "1350/1350 [==============================] - 0s 98us/step\n",
            ">1.000\n",
            "--- fus_mlp ---\n",
            "1350/1350 [==============================] - 0s 98us/step\n",
            ">1.000\n",
            "--- fus_mlp ---\n",
            "1350/1350 [==============================] - 0s 97us/step\n",
            ">0.936\n",
            "Estimated Accuracy 0.987 (0.025)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7XzPcWKwAQO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "# function to save keras model on google drive\n",
        "def save_model(model, model_name):\n",
        "    # create on colab directory\n",
        "    model.save(model_name)    \n",
        "    model_file = drive.CreateFile({'title': model_name})\n",
        "    model_file.SetContentFile(model_name)\n",
        "    model_file.Upload()\n",
        "\n",
        "    # download to google drive\n",
        "    drive.CreateFile({'id': model_file.get('id')})\n",
        "    \n",
        "# save keras models on google drive\n",
        "save_model(model_emg, name_emg)\n",
        "save_model(model_dvs_evs, name_dvs_evs)\n",
        "save_model(model_dav_evs, name_dav_evs)\n",
        "save_model(model_dav_aps, name_dav_aps)\n",
        "save_model(model_emg_dvs_evs, name_emg_dvs_evs)\n",
        "save_model(model_emg_dav_evs, name_emg_dav_evs)\n",
        "save_model(model_emg_dav_aps, name_emg_dav_aps)\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N48CVsymUUEm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "# links for models loading\n",
        "link_emg = \"https://drive.google.com/open?id=1-47ifKsKx5PFg4IgcsjGmEoOFwcG5or-\"\n",
        "link_dvs_evs = \"https://drive.google.com/open?id=1OrUHw8d0XGrcjlYSCrXpSigCEm0Ck30v\"\n",
        "link_dav_evs = \"https://drive.google.com/open?id=1XeQUUTnPcJxJDVy8N17HG93oDpoprn7q\"\n",
        "link_dav_aps = \"https://drive.google.com/open?id=1SQpQ2_ncZsKdX6RDcGQnrfvKdzIFnfSW\"\n",
        "link_emg_dvs_evs = \"https://drive.google.com/open?id=1pU8kev4Aj5D6YapvRErqPmMEBVlPdlOi\"\n",
        "link_emg_dav_evs = \"https://drive.google.com/open?id=1NxQi-51-WgvVsnrGp1jur8mL9yghzzJd\"\n",
        "link_emg_dav_aps = \"https://drive.google.com/open?id=1lXCSsP_vRqi4d5tisoSNVXE58Lgc5kUA\"\n",
        "\n",
        "# names for models loading\n",
        "name_emg = \"cnn_200_emg.h5\"\n",
        "name_dvs_evs = \"cnn_200_dvs_evs.h5\"\n",
        "name_dav_evs = \"cnn_200_dav_evs.h5\"\n",
        "name_dav_aps = \"cnn_200_dav_aps.h5\"\n",
        "name_emg_dvs_evs = \"cnn_200_emg+dvs_evs.h5\"\n",
        "name_emg_dav_evs = \"cnn_200_emg+dav_evs.h5\"\n",
        "name_emg_dav_aps = \"cnn_200_emg+dav_aps.h5\"\n",
        "\n",
        "# fuction to load h5 models from google drive\n",
        "def load_h5_model(link, name):\n",
        "    fluff, id = link.split('=')\n",
        "    print(\"--- loading model: \" + name + \" from id -> \" + id + \" ---\")\n",
        "    downloaded = drive.CreateFile({\"id\":id})\n",
        "    downloaded.GetContentFile(name)  \n",
        "    model = load_model(name)\n",
        "\n",
        "    #model.summary()\n",
        "  \n",
        "# load models\n",
        "load_h5_model(link_emg, name_emg)\n",
        "load_h5_model(link_dvs_evs, name_dvs_evs)\n",
        "load_h5_model(link_dav_evs, name_dav_evs)\n",
        "load_h5_model(link_dav_aps, name_dav_aps)\n",
        "load_h5_model(link_emg_dvs_evs, name_emg_dvs_evs)\n",
        "load_h5_model(link_emg_dav_evs, name_emg_dav_evs)\n",
        "load_h5_model(link_emg_dav_aps, name_emg_dav_aps)\n",
        "\n",
        "# function to convert h5 model to tflite model\n",
        "def save_tflite(name, file_name):\n",
        "    print(\"--- saving model: \" + name + \" into -> \" + file_name + \" ---\")\n",
        "    converter = lite.TFLiteConverter.from_keras_model_file(name)\n",
        "    model_tflite = converter.convert()\n",
        "    file = open(file_name, 'wb')\n",
        "    file.write(model_tflite)\n",
        "\n",
        "    # save tflite model on google drive\n",
        "    # create on colab directory\n",
        "    model_file = drive.CreateFile({'title': file_name})\n",
        "    model_file.SetContentFile(file_name)\n",
        "    model_file.Upload()\n",
        "\n",
        "    # download to google drive\n",
        "    drive.CreateFile({'id': model_file.get('id')})\n",
        "    \n",
        "# convert h5 model to tflite model\n",
        "save_tflite(name_emg, tflite_emg)\n",
        "save_tflite(name_dvs_evs, tflite_dvs_evs)\n",
        "save_tflite(name_dav_evs, tflite_dav_evs)\n",
        "save_tflite(name_dav_aps, tflite_dav_aps)\n",
        "save_tflite(name_emg_dvs_evs, tflite_emg_dvs_evs)\n",
        "save_tflite(name_emg_dav_evs, tflite_emg_dav_evs)\n",
        "save_tflite(name_emg_dav_aps, tflite_emg_dav_aps)\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}